{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World\n",
    "\n",
    "|       | reward + 1 | Penalty -1 |\n",
    "|-------|------------|------------|\n",
    "|       |            |            |\n",
    "|       | wall       |            |\n",
    "| start |            |            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1. Write a code in Python implementing Value iteration for a grid world given in the\n",
    "image above\n",
    "Values you will require:\n",
    "1. The reward for reaching the goal state = 1\n",
    "2. The penalty for reaching the red state = -1\n",
    "3. Step cost = -0.04\n",
    "4. Probability of going in the direction of the action = 0.7\n",
    "5. Probability of going in a direction perpendicular to the action = 0.15\n",
    "Print the utility value of each cell in the grid after each iteration until the values\n",
    "converge. (Assume the values converge when the difference between the utilities\n",
    "for each cell is <= 0.0001)\n",
    "Note: The agent does not change its state if it hits a wall or the boundaries\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04       1.        -1.       ]\n",
      " [-0.046      0.6531     0.26717  ]\n",
      " [-0.0469     0.         0.147019 ]\n",
      " [ 0.        -0.04       0.0569133]]\n"
     ]
    }
   ],
   "source": [
    "# latest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the grid world\n",
    "WORLD = np.array([[0, 1, -1],\n",
    "                  [0, 0, 0],\n",
    "                  [0, float('-inf'), 0],\n",
    "                  [float('-inf'), 0, 0]])\n",
    "# print(WORLD)\n",
    "\n",
    "# Define the action set\n",
    "ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "# Define the probabilities for action and perpendicular action\n",
    "PROB_ACTION = 0.7\n",
    "PROB_PERP_ACTION = 0.15\n",
    "\n",
    "# Define the rewards and penalties\n",
    "REWARD_GOAL = 1\n",
    "PENALTY_RED = -1\n",
    "STEP_COST = -0.04\n",
    "\n",
    "# Define the convergence threshold\n",
    "EPSILON = 0.0001\n",
    "\n",
    "# Function to check if a state is valid\n",
    "def is_valid_state(state, world):\n",
    "    row, col = state\n",
    "    rows, cols = world.shape\n",
    "    if row < 0 or row >= rows or col < 0 or col >= cols or world[row, col] == float('-inf'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Initialize utility values for each cell in the grid\n",
    "utilities = np.zeros_like(WORLD)\n",
    "\n",
    "# Perform Value Iteration\n",
    "delta = float('inf')\n",
    "while delta > EPSILON:\n",
    "    delta = 0\n",
    "    for row in range(WORLD.shape[0]):\n",
    "        for col in range(WORLD.shape[1]):\n",
    "            if not is_valid_state((row, col), WORLD):\n",
    "                continue\n",
    "\n",
    "            prev_utility = utilities[row, col]\n",
    "\n",
    "            max_action_utility = float('-inf')\n",
    "            for action in ACTIONS:\n",
    "                next_row, next_col = row, col\n",
    "                if action == 'UP':\n",
    "                    next_row -= 1\n",
    "                elif action == 'DOWN':\n",
    "                    next_row += 1\n",
    "                elif action == 'LEFT':\n",
    "                    next_col -= 1\n",
    "                elif action == 'RIGHT':\n",
    "                    next_col += 1\n",
    "\n",
    "                next_state = (next_row, next_col)\n",
    "\n",
    "                # Calculate the estimated utility for the current action\n",
    "                estimated_utility = 0\n",
    "                for act in ACTIONS:\n",
    "                    if act == action:\n",
    "                        prob = PROB_ACTION\n",
    "                    else:\n",
    "                        prob = PROB_PERP_ACTION\n",
    "                    next_row, next_col = row, col\n",
    "                    if act == 'UP':\n",
    "                        next_row -= 1\n",
    "                    elif act == 'DOWN':\n",
    "                        next_row += 1\n",
    "                    elif act == 'LEFT':\n",
    "                        next_col -= 1\n",
    "                    elif act == 'RIGHT':\n",
    "                        next_col += 1\n",
    "\n",
    "                    next_state = (next_row, next_col)\n",
    "                    if is_valid_state(next_state, WORLD):\n",
    "                        estimated_utility += prob * utilities[next_row, next_col]\n",
    "                    else:\n",
    "                        estimated_utility += prob * prev_utility\n",
    "\n",
    "                # Update the maximum action utility value\n",
    "                max_action_utility = max(max_action_utility, estimated_utility)\n",
    "\n",
    "            # Update the utility value for the current state\n",
    "            if WORLD[row, col] == REWARD_GOAL:\n",
    "                utilities[row, col] = REWARD_GOAL\n",
    "            elif WORLD[row, col] == PENALTY_RED:\n",
    "                utilities[row, col] = PENALTY_RED\n",
    "            else:\n",
    "                utilities[row, col] = STEP_COST + max_action_utility\n",
    "    print(utilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_21384\\2965731931.py:51: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  policy = np.empty_like(WORLD, dtype=np.object)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m             estimated_utility \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prob \u001b[39m*\u001b[39m utilities[next_row, next_col]\n\u001b[0;32m     81\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m             estimated_utility \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prob \u001b[39m*\u001b[39;49m prev_utility\n\u001b[0;32m     84\u001b[0m     \u001b[39m# Update the maximum action utility value and the best action\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39mif\u001b[39;00m estimated_utility \u001b[39m>\u001b[39m max_action_utility:\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid world\n",
    "WORLD = np.array([[0, 1, -1],\n",
    "                  [0, 0, 0],\n",
    "                  [0, float('-inf'), 0],\n",
    "                  ['x', 0, 0]])\n",
    "\n",
    "# Define the action set\n",
    "ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "# Define the probabilities for action and perpendicular action\n",
    "PROB_ACTION = 0.7\n",
    "PROB_PERP_ACTION = 0.15\n",
    "\n",
    "# Define the rewards and penalties\n",
    "REWARD_GOAL = 1\n",
    "PENALTY_RED = -1\n",
    "STEP_COST = -0.04\n",
    "\n",
    "# Define the convergence threshold\n",
    "EPSILON = 0.0001\n",
    "\n",
    "# Function to check if a state is valid\n",
    "def is_valid_state(state, world):\n",
    "    \"\"\"\n",
    "    Check if a state is valid (within the bounds of the grid).\n",
    "    Args:\n",
    "        state (tuple): Current state (row, col) in the grid.\n",
    "        world (numpy.ndarray): Grid world representation.\n",
    "    Returns:\n",
    "        bool: True if the state is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    rows, cols = world.shape\n",
    "    if row < 0 or row >= rows or col < 0 or col >= cols or world[row, col] == float('-inf'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Initialize utility values for each cell in the grid\n",
    "utilities = np.zeros_like(WORLD)\n",
    "\n",
    "# Set the start state\n",
    "start_state = (3, 0)\n",
    "\n",
    "# Perform Value Iteration\n",
    "delta = float('inf')\n",
    "while delta > EPSILON:\n",
    "    delta = 0\n",
    "    i = 0\n",
    "    policy = np.empty_like(WORLD, dtype=np.object)\n",
    "    for row in range(WORLD.shape[0]):\n",
    "        for col in range(WORLD.shape[1]):\n",
    "            if not is_valid_state((row, col), WORLD):\n",
    "                policy[row, col] = 'WALL'\n",
    "                continue\n",
    "\n",
    "            prev_utility = utilities[row, col]\n",
    "\n",
    "            max_action_utility = float('-inf')\n",
    "            best_action = None\n",
    "            estimated_utility = 0\n",
    "            for act in ACTIONS:\n",
    "                    if act == action:\n",
    "                        prob = PROB_ACTION\n",
    "                    else:\n",
    "                        prob = PROB_PERP_ACTION\n",
    "                    next_row, next_col = row, col\n",
    "                    if act == 'UP':\n",
    "                        next_row -= 1\n",
    "                    elif act == 'DOWN':\n",
    "                        next_row += 1\n",
    "                    elif act == 'LEFT':\n",
    "                        next_col -= 1\n",
    "                    elif act == 'RIGHT':\n",
    "                        next_col += 1\n",
    "\n",
    "                    next_state = (next_row, next_col)\n",
    "                    if is_valid_state(next_state, WORLD):\n",
    "                        estimated_utility += prob * utilities[next_row, next_col]\n",
    "                    else:\n",
    "                        estimated_utility += prob * prev_utility\n",
    "\n",
    "                # Update the maximum action utility value and the best action\n",
    "            if estimated_utility > max_action_utility:\n",
    "                    max_action_utility = estimated_utility\n",
    "                    best_action = action\n",
    "            policy[row, col] = best_action\n",
    "\n",
    "            # Update the utility value for the current state\n",
    "        if WORLD[row, col] == REWARD_GOAL:\n",
    "                utilities[row, col] = REWARD_GOAL\n",
    "        elif WORLD[row, col] == PENALTY_RED:\n",
    "                utilities[row, col] = PENALTY_RED\n",
    "        else:\n",
    "                utilities[row, col] = STEP_COST + max_action_utility\n",
    "\n",
    "            # Update the delta (maximum change in utility value)\n",
    "        delta = max(delta, abs(prev_utility - utilities[row, col]))\n",
    "\n",
    "    # Print the utility values after each iteration\n",
    "        print(\"Iteration: \", i + 1)\n",
    "        i += 1 \n",
    "    print(utilities)\n",
    "\n",
    "    # Check for convergence\n",
    "    if delta <= EPSILON:\n",
    "        print(\"Convergence achieved after\", i + 1, \"iterations.\")\n",
    "        break\n",
    "'''\n",
    "# Print the final policy\n",
    "policy = np.empty_like(WORLD, dtype=np.object)\n",
    "for row in range(WORLD.shape[0]):\n",
    "    for col in range(WORLD.shape[1]):\n",
    "        if not is_valid_state((row, col), WORLD):\n",
    "            policy[row, col] = 'WALL'\n",
    "        else:\n",
    "            max_action_utility = float('-inf')\n",
    "            best_action = None\n",
    "            for action in ACTIONS:\n",
    "                next_row, next_col = row, col\n",
    "                if action == 'UP':\n",
    "                    next_row -= 1\n",
    "                elif action == 'DOWN':\n",
    "                    next_row += 1\n",
    "                elif action == 'LEFT':\n",
    "                    next_col -= 1\n",
    "                elif action == 'RIGHT':\n",
    "                    next_col += 1\n",
    "\n",
    "                next_state = (next_row, next_col)\n",
    "\n",
    "                # Calculate the estimated utility for the current action\n",
    "                estimated_utility = 0\n",
    "                for act in ACTIONS:\n",
    "                    if act == action:\n",
    "                        prob = PROB_ACTION\n",
    "                    else:\n",
    "                        prob = PROB_PERP_ACTION\n",
    "                    next_row, next_col = row, col\n",
    "                    if act == 'UP':\n",
    "                        next_row -= 1\n",
    "                    elif act == 'DOWN':\n",
    "                        next_row += 1\n",
    "                    elif act == 'LEFT':\n",
    "                        next_col -= 1\n",
    "                    elif act == 'RIGHT':\n",
    "                        next_col += 1\n",
    "\n",
    "                    next_state = (next_row, next_col)\n",
    "                    if is_valid_state(next_state, WORLD):\n",
    "                        estimated_utility += prob * utilities[next_row, next_col]\n",
    "                    else:\n",
    "                        estimated_utility += prob * utilities[row, col]\n",
    "\n",
    "                # Update the maximum action utility value and the best action\n",
    "                if estimated_utility > max_action_utility:\n",
    "                    max_action_utility = estimated_utility\n",
    "                    best_action = action\n",
    "\n",
    "            # Set the best action as the policy for the current state\n",
    "            policy[row, col] = best_action\n",
    "'''\n",
    "# Print the final policy\n",
    "print(\"Final Policy:\")\n",
    "print(policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
